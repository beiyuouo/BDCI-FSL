# training hyperparameters
encoder_lr: !!float 20e-6
decoder_lr: !!float 20e-6
eps: !!float 1e-6
betas: [0.9, 0.999]
weight_decay: 0.0
num_cycles: 0.5
num_warmup_steps: 0
max_input_length: 1024
max_grad_norm: 500

scheduler: "cosine"
batch_scheduler: True

epochs: 50
batch_size: 8

# pretrain config
pretrain:
  epochs: 50
  batch_size: 8
  lr: !!float 1e-5
  weight_decay: 0.0

# mlm config
mlm_probability: 0.15
special_tokens_mask:
prob_replace_mask: 0.8
prob_replace_rand: 0.1
prob_keep_ori: 0.1

# model config
model_name: "hfl/chinese-roberta-wwm-ext"
num_labels: 36

# data config
data_path: "./../data"
num_workers: 4

# path settings
run_path: "./../runs"
exp_name: "exp"
export_path: "./../export"

# misc
seed: 3407
chpt_freq: 10
log_freq: 20
use_best: False
from_scratch: False
with_augment: False
use_tta: False

# ssl
ssl:
  lambda_u: 100
  T: 0.5
  K: 2
  alpha: 0.75

# device
device: "cuda"
